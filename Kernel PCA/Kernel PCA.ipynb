{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a58364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # importing pandas\n",
    "import numpy as np # importing numpy\n",
    "import matplotlib.pyplot as plt  # importing matplotlib\n",
    "from sklearn.decomposition import KernelPCA #importing kernelpca\n",
    "from sklearn.metrics import r2_score #importing r2 score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "movie=pd.read_csv('ml360datasets/Movie_classification.csv') #reading the movie classification data\n",
    "house=pd.read_csv('C:/Users/Administrator/Downloads/ml360datasets/kc_house.csv') # reading the house data\n",
    "loan=pd.read_csv('C:/Users/Administrator/Downloads/ml360datasets/loan_classification.csv') #reading the loan classification\n",
    "wine=pd.read_csv('C:/Users/Administrator/Downloads/ml360datasets/winequality-white.csv') #reading the wine data\n",
    "iris=pd.read_csv('C:/Users/Administrator/Downloads/ml360datasets/Iris.csv') #reading the iris data\n",
    "insurance=pd.read_csv('C:/Users/Administrator/Downloads/ml360datasets/insurance.csv') #reading the iris data\n",
    "\n",
    "def null_replace(df): #define fun for replacing the null values\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtypes == \"object\"): # checking the datatype\n",
    "            df[col]=df[col].fillna(df[col].mode()[0]) #replacing the null values for catgeory column\n",
    "        else:\n",
    "            df[col]=df[col].fillna(df[col].mean()) # replacing the null values for numerical columns\n",
    "    return df\n",
    "\n",
    "def label_encoder(df): #define fun for label encoder \n",
    "    from sklearn.preprocessing import LabelEncoder #doing the labelencoder for category columns\n",
    "    le = LabelEncoder() # giving variable for labelencoder\n",
    "    for col in df.columns:  \n",
    "        if (df[col].dtypes == \"object\"): #checking the data type of the column\n",
    "            df[col] = df[col].astype(str) \n",
    "            df[col] = le.fit_transform(df[col]) #applying the label encoder\n",
    "    return df #returns dataframe \n",
    "\n",
    "\n",
    "house=null_replace(house) #appying the function for replacing null values\n",
    "loan=null_replace(loan)  #appying the function for replacing null values\n",
    "wine=null_replace(wine)  #appying the function for replacing null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f957eee",
   "metadata": {},
   "source": [
    "# In kernel pca we trying to optimize N components , kernel and Gamma values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_components we are trying to identify by the cummulative varience explained. we are taking nearest value of 95(which occupies\n",
    "#max variance). the nearest value of 95 variance that index point we are considering it has N_components\n",
    "\n",
    "\n",
    "#In Kernel we have different types of them to decide we are calculatng the reconstruction error of Original and inverse \n",
    "#transfrommed features the difference between them which ever kernel is low will choose that kernel\n",
    "\n",
    "#In Gamma value only specific kerenl will consider ['poly','rbf', 'sigmoid'] for these kernel the optimal value is \n",
    "#1 / (len(X.columns) * X.values.var())\n",
    "\n",
    "#for remaining kernel they will ignore the gamma value in that case the default gamma value will be 1/len(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5819df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As per my observation The kernel Pca is working for classification data not on regression data \n",
    "#for classification I took Movie, Loan , Iris,wine\n",
    "#for Regression Wine, house, insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25d6f9",
   "metadata": {},
   "source": [
    "# Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7b3080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [42.187448758298764, 11.255143435368113, 6.517298747176463, 5.8258940686909835, 5.451824486345361, 5.290035849717573, 5.074730219686693, 4.419500183710625, 4.136709602334205, 3.101133602095518, 2.1894937006237005, 1.5637588131952458, 1.3270823433819776, 1.1810573027811653, 0.4135391232374326, 0.04281361642512702, 0.012224637176785525, 0.010311509754266958]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 42.18744876  53.44259219  59.95989094  65.78578501  71.2376095\n",
      "  76.52764535  81.60237557  86.02187575  90.15858535  93.25971895\n",
      "  95.44921265  97.01297147  98.34005381  99.52111111  99.93465024\n",
      "  99.97746385  99.98968849 100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [132.59771577319376, 126.67090213246853, 113.79765444432853, 105.76283782688972, 128.02851394690126]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.46      0.49        65\n",
      "           1       0.63      0.68      0.65        87\n",
      "\n",
      "    accuracy                           0.59       152\n",
      "   macro avg       0.57      0.57      0.57       152\n",
      "weighted avg       0.58      0.59      0.58       152\n",
      "\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.54        65\n",
      "           1       0.65      0.62      0.64        87\n",
      "\n",
      "    accuracy                           0.59       152\n",
      "   macro avg       0.59      0.59      0.59       152\n",
      "weighted avg       0.60      0.59      0.59       152\n",
      "\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.52        65\n",
      "           1       0.64      0.62      0.63        87\n",
      "\n",
      "    accuracy                           0.58       152\n",
      "   macro avg       0.57      0.57      0.57       152\n",
      "weighted avg       0.58      0.58      0.58       152\n",
      "\n",
      "\n",
      "\n",
      "KernelPca sigmoid Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56        65\n",
      "           1       0.67      0.66      0.66        87\n",
      "\n",
      "    accuracy                           0.62       152\n",
      "   macro avg       0.61      0.61      0.61       152\n",
      "weighted avg       0.62      0.62      0.62       152\n",
      "\n",
      "\n",
      "\n",
      "KernelPca cosine Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.51        65\n",
      "           1       0.64      0.67      0.65        87\n",
      "\n",
      "    accuracy                           0.59       152\n",
      "   macro avg       0.58      0.58      0.58       152\n",
      "weighted avg       0.59      0.59      0.59       152\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie=null_replace(movie) #appying the function for replacing null values\n",
    "movie=label_encoder(movie) #appying the function for label encoder\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=movie.drop('Start_Tech_Oscar',axis=1) # storing the X matrix \n",
    "y=movie['Start_Tech_Oscar'] # defing the y as target variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=movie.drop('Start_Tech_Oscar',axis=1) #deffing the X features \n",
    "y=movie['Start_Tech_Oscar'] #deffing the target variable \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the standard scaler\n",
    "sc = StandardScaler() #defing the standard scaler\n",
    "X_train = sc.fit_transform(X) #ftting and transfroming the X features\n",
    "\n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train) #fitting and transforming the X features\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca Linear Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=movie.drop('Start_Tech_Oscar',axis=1) #storing the X features \n",
    "y=movie['Start_Tech_Oscar'] # defining the target variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the standard scaler \n",
    "sc = StandardScaler()  #defining the standardscaler\n",
    "X_train = sc.fit_transform(X) #fit and transforming the X features \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca poly Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=movie.drop('Start_Tech_Oscar',axis=1) #storing the X features \n",
    "y=movie['Start_Tech_Oscar'] # defining the target variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the standard scaler \n",
    "sc = StandardScaler()  #defining the standardscaler\n",
    "X_train = sc.fit_transform(X) #fit and transforming the X features \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca rbf Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=movie.drop('Start_Tech_Oscar',axis=1) #storing the X features \n",
    "y=movie['Start_Tech_Oscar'] # defining the target variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the standard scaler \n",
    "sc = StandardScaler()  #defining the standardscaler\n",
    "X_train = sc.fit_transform(X) #fit and transforming the X features \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca sigmoid Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=movie.drop('Start_Tech_Oscar',axis=1) #storing the X features \n",
    "y=movie['Start_Tech_Oscar'] # defining the target variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the standard scaler \n",
    "sc = StandardScaler()  #defining the standardscaler\n",
    "X_train = sc.fit_transform(X) #fit and transforming the X features \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca cosine Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#we can check the reconstruction error is less for sigmoid and we got better results for sigmoid kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787fb60",
   "metadata": {},
   "source": [
    "# Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d25c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [15.623882996887438, 12.229427600969263, 10.114824810762203, 9.252034660414994, 8.586944365899377, 8.324837184372615, 7.654349575594069, 7.072443596299781, 6.929544893743198, 6.469504079041541, 4.847404575367141, 2.8948016606483766]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 15.623883    27.8533106   37.96813541  47.22017007  55.80711443\n",
      "  64.13195162  71.78630119  78.85874479  85.78828968  92.25779376\n",
      "  97.10519834 100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [118.93528231946378, 110.71571835329996, 102.71549023290447, 92.84844034936181, 114.50515757751825]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.51      0.62        65\n",
      "           1       0.78      0.93      0.85       120\n",
      "\n",
      "    accuracy                           0.78       185\n",
      "   macro avg       0.79      0.72      0.74       185\n",
      "weighted avg       0.79      0.78      0.77       185\n",
      "\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.52      0.64        65\n",
      "           1       0.78      0.94      0.86       120\n",
      "\n",
      "    accuracy                           0.79       185\n",
      "   macro avg       0.81      0.73      0.75       185\n",
      "weighted avg       0.80      0.79      0.78       185\n",
      "\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.48      0.58        65\n",
      "           1       0.76      0.91      0.83       120\n",
      "\n",
      "    accuracy                           0.76       185\n",
      "   macro avg       0.75      0.69      0.70       185\n",
      "weighted avg       0.75      0.76      0.74       185\n",
      "\n",
      "\n",
      "\n",
      "KernelPca Sigmoid Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.52      0.62        65\n",
      "           1       0.78      0.92      0.84       120\n",
      "\n",
      "    accuracy                           0.78       185\n",
      "   macro avg       0.78      0.72      0.73       185\n",
      "weighted avg       0.78      0.78      0.77       185\n",
      "\n",
      "\n",
      "\n",
      "KernelPca Cosine Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.43      0.55        65\n",
      "           1       0.75      0.93      0.83       120\n",
      "\n",
      "    accuracy                           0.76       185\n",
      "   macro avg       0.76      0.68      0.69       185\n",
      "weighted avg       0.76      0.76      0.73       185\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan=null_replace(loan) #appying the function for replacing null values\n",
    "loan=label_encoder(loan) #appying the function for label encoder\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=loan.drop('Loan_Status',axis=1) #storing the X features \n",
    "y=loan['Loan_Status'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=loan.drop('Loan_Status',axis=1) #storing the X features \n",
    "y=loan['Loan_Status'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train) #fitting and transforming the X features\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca Linear Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=loan.drop('Loan_Status',axis=1) #storing the X features \n",
    "y=loan['Loan_Status'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca poly Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=loan.drop('Loan_Status',axis=1) #storing the X features \n",
    "y=loan['Loan_Status'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var())) # defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca rbf Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=loan.drop('Loan_Status',axis=1) #storing the X features \n",
    "y=loan['Loan_Status'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca Sigmoid Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=loan.drop('Loan_Status',axis=1) #storing the X features \n",
    "y=loan['Loan_Status'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with cosine and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca Cosine Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72406f5",
   "metadata": {},
   "source": [
    "# Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bca129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [74.70533000277261, 18.435256887931974, 4.682624420508823, 1.7647673742565004, 0.41202131453009]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 74.70533     93.14058689  97.82321131  99.58797869 100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [36.17036517202373, 36.89625093647699, 34.27594225580259, 33.46060298114652, 34.2012999236542]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "\n",
      "KernelPca Sigmoid Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "\n",
      "KernelPca cosine Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris=null_replace(iris)\n",
    "iris=label_encoder(iris)\n",
    "\n",
    "iris=null_replace(iris) #appying the function for replacing null values\n",
    "iris=label_encoder(iris) #appying the function for label encoder\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=iris.drop('Species',axis=1)  #storing the X features \n",
    "y=iris['Species'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=iris.drop('Species',axis=1)  #storing the X features \n",
    "y=iris['Species'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca Linear Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=iris.drop('Species',axis=1)\n",
    "y=iris['Species']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca poly Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=iris.drop('Species',axis=1)\n",
    "y=iris['Species']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var())) # defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca rbf Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=iris.drop('Species',axis=1)\n",
    "y=iris['Species']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca Sigmoid Model Results','\\n',classification_report(y_test, y_pred)) #printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=iris.drop('Species',axis=1)\n",
    "y=iris['Species']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with cosine and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report #importing the classification report\n",
    "print('KernelPca cosine Model Results','\\n',classification_report(y_test, y_pred))#printing the classification report\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02fc9f",
   "metadata": {},
   "source": [
    "# Wine - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9c5343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [29.29321718754935, 14.320363009824618, 11.106103089574539, 9.259294133372196, 8.84849617879475, 8.534013740313142, 6.605436572268536, 5.448713451289449, 3.7649424158637173, 2.6317012597137124, 0.18771896143598701]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 29.29321719  43.6135802   54.71968329  63.97897742  72.8274736\n",
      "  81.36148734  87.96692391  93.41563736  97.18057978  99.81228104\n",
      " 100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [318.21191534955653, 306.298271654715, 295.49157949581024, 303.89915616908604, 308.36087301471014]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.50      0.14      0.22         7\n",
      "           4       0.50      0.15      0.23        40\n",
      "           5       0.68      0.70      0.69       426\n",
      "           6       0.66      0.77      0.71       668\n",
      "           7       0.68      0.52      0.59       280\n",
      "           8       0.90      0.39      0.54        49\n",
      "\n",
      "    accuracy                           0.67      1470\n",
      "   macro avg       0.65      0.44      0.50      1470\n",
      "weighted avg       0.67      0.67      0.66      1470\n",
      "\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.50      0.14      0.22         7\n",
      "           4       0.50      0.15      0.23        40\n",
      "           5       0.66      0.69      0.68       426\n",
      "           6       0.65      0.75      0.70       668\n",
      "           7       0.71      0.55      0.62       280\n",
      "           8       0.90      0.37      0.52        49\n",
      "\n",
      "    accuracy                           0.66      1470\n",
      "   macro avg       0.65      0.44      0.49      1470\n",
      "weighted avg       0.67      0.66      0.65      1470\n",
      "\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.50      0.14      0.22         7\n",
      "           4       0.46      0.15      0.23        40\n",
      "           5       0.66      0.68      0.67       426\n",
      "           6       0.64      0.76      0.70       668\n",
      "           7       0.71      0.51      0.60       280\n",
      "           8       0.90      0.37      0.52        49\n",
      "\n",
      "    accuracy                           0.66      1470\n",
      "   macro avg       0.65      0.44      0.49      1470\n",
      "weighted avg       0.66      0.66      0.65      1470\n",
      "\n",
      "\n",
      "\n",
      "KernelPca sigmoid Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.50      0.14      0.22         7\n",
      "           4       0.43      0.15      0.22        40\n",
      "           5       0.66      0.68      0.67       426\n",
      "           6       0.63      0.75      0.69       668\n",
      "           7       0.69      0.53      0.60       280\n",
      "           8       0.90      0.37      0.52        49\n",
      "\n",
      "    accuracy                           0.65      1470\n",
      "   macro avg       0.64      0.44      0.49      1470\n",
      "weighted avg       0.66      0.65      0.64      1470\n",
      "\n",
      "\n",
      "\n",
      "KernelPca cosine Model Results \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.80      0.10      0.18        40\n",
      "           5       0.66      0.68      0.67       426\n",
      "           6       0.63      0.75      0.69       668\n",
      "           7       0.67      0.50      0.57       280\n",
      "           8       0.90      0.39      0.54        49\n",
      "\n",
      "    accuracy                           0.65      1470\n",
      "   macro avg       0.61      0.40      0.44      1470\n",
      "weighted avg       0.66      0.65      0.64      1470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "wine=null_replace(wine) #appying the function for replacing null values\n",
    "wine=label_encoder(wine) #appying the function for label encoder\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)  #storing the X features \n",
    "y=wine['quality'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)  #storing the X features \n",
    "y=wine['quality'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier () # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import  classification_report  #importing the classification_report\n",
    "print('KernelPca Linear Model Results','\\n', classification_report(y_test, y_pred))#printing the classification_report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier () # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import  classification_report\n",
    "print('KernelPca poly Model Results','\\n', classification_report(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var())) # defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier () # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import  classification_report\n",
    "print('KernelPca rbf Model Results','\\n', classification_report(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier () # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import  classification_report\n",
    "print('KernelPca sigmoid Model Results','\\n', classification_report(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with cosine and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   #importing the randomforest\n",
    "\n",
    "classifier= RandomForestClassifier () # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import  classification_report\n",
    "print('KernelPca cosine Model Results','\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4121a5e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c8675",
   "metadata": {},
   "source": [
    "# Wine Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6fa6317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [29.29321718754935, 14.320363009824618, 11.106103089574539, 9.259294133372196, 8.84849617879475, 8.534013740313142, 6.605436572268536, 5.448713451289449, 3.7649424158637173, 2.6317012597137124, 0.18771896143598701]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 29.29321719  43.6135802   54.71968329  63.97897742  72.8274736\n",
      "  81.36148734  87.96692391  93.41563736  97.18057978  99.81228104\n",
      " 100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [318.2119153495568, 306.2982716548172, 295.49157949581024, 303.8991561690859, 308.3608730147102]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      " 0.4823779543243877\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      " 0.49186766134513915\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      " 0.4868597204251912\n",
      "\n",
      "\n",
      "KernelPca sigmoid Model Results \n",
      " 0.4903269782274393\n",
      "\n",
      "\n",
      "KernelPca cosine Model Results \n",
      " 0.47162533763926207\n"
     ]
    }
   ],
   "source": [
    "wine=null_replace(wine) #appying the function for replacing null values\n",
    "wine=label_encoder(wine) #appying the function for label encoder\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)  #storing the X features \n",
    "y=wine['quality'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)  #storing the X features \n",
    "y=wine['quality'] # defining the target variable\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score  #importing the classification report\n",
    "print('KernelPca Linear Model Results','\\n',r2_score(y_test, y_pred))#printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca poly Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var())) # defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca rbf Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca sigmoid Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=wine.drop('quality',axis=1)\n",
    "y=wine['quality']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with cosine and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca cosine Model Results','\\n',r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e7d17",
   "metadata": {},
   "source": [
    "# House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "937d98b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [31.655100740264388, 12.248609345935519, 7.81593763856578, 7.153855385437388, 6.088862294193866, 5.511587674422788, 4.7853504350649505, 4.223966417817956, 3.8853622380728536, 3.256449811660221, 2.951799419841878, 2.4369512527287784, 2.1206225004036168, 1.6215300438108184, 1.5037532593074445, 1.1086680563537676, 0.9058499749916994, 0.7257435111262726, 8.314614143677452e-16]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 31.65510074  43.90371009  51.71964772  58.87350311  64.9623654\n",
      "  70.47395308  75.25930351  79.48326993  83.36863217  86.62508198\n",
      "  89.5768814   92.01383265  94.13445515  95.7559852   97.25973846\n",
      "  98.36840651  99.27425649 100.         100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [191.89243752823918, 182.45269569687068, 166.02585661172063, 153.69587537532584, 185.34911150559557]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      " 0.4954749648705049\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      " -0.00907653180427559\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      " -0.007700482523640417\n",
      "\n",
      "\n",
      "KernelPca sigmoid Model Results \n",
      " -0.008259377127916467\n",
      "\n",
      "\n",
      "KernelPca cosine Model Results \n",
      " 0.4533917722986438\n"
     ]
    }
   ],
   "source": [
    "house=null_replace(house)\n",
    "house=label_encoder(house)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=house.drop('sqft_lot15',axis=1)\n",
    "y=house['sqft_lot15']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=house.drop('sqft_lot15',axis=1)\n",
    "y=house['sqft_lot15']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score  #importing the classification report\n",
    "print('KernelPca Linear Model Results','\\n',r2_score(y_test, y_pred))#printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=house.drop('sqft_lot15',axis=1)\n",
    "y=house['sqft_lot15']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca poly Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=house.drop('sqft_lot15',axis=1)\n",
    "y=house['sqft_lot15']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var())) # defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca rbf Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=house.drop('sqft_lot15',axis=1)\n",
    "y=house['sqft_lot15']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca sigmoid Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=house.drop('sqft_lot15',axis=1)\n",
    "y=house['sqft_lot15']\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with cosine and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca cosine Model Results','\\n',r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b3fed",
   "metadata": {},
   "source": [
    "# Medical Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24eda903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by each component is \n",
      " [20.060309157445776, 18.136640229738703, 16.992313681861653, 16.087702325770536, 15.399424412600704, 13.323610192582619]\n",
      "----------------------------------------\n",
      "Cumulative variance captured as we travel each component \n",
      " [ 20.06030916  38.19694939  55.18926307  71.27696539  86.67638981\n",
      " 100.        ]\n",
      "\n",
      "\n",
      "reconstruction_error [122.37806762289244, 122.98626428770282, 116.95900766908521, 114.86088234283275, 120.64109409880736]\n",
      "\n",
      "\n",
      "KernelPca Linear Model Results \n",
      " 0.8100594308937372\n",
      "\n",
      "\n",
      "KernelPca poly Model Results \n",
      " 0.818338631774253\n",
      "\n",
      "\n",
      "KernelPca rbf Model Results \n",
      " 0.8049215756105762\n",
      "\n",
      "\n",
      "KernelPca sigmoid Model Results \n",
      " 0.8077429084708132\n",
      "\n",
      "\n",
      "KernelPca cosine Model Results \n",
      " 0.813439273423118\n"
     ]
    }
   ],
   "source": [
    "insurance=null_replace(insurance)\n",
    "insurance=label_encoder(insurance)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=insurance.drop('charges',axis=1)\n",
    "y=insurance['charges']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "X_covariance_matrix = np.cov(X_train.T) #calculating the covariance for Transformed X matrix \n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_covariance_matrix) #calculating the eigenvales and eigen vectors\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "tot = sum(eig_vals) #calculating the sum of eigen values\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)] #soring the eigenvalues and calculating the variance explained\n",
    "cum_var_exp = np.cumsum(var_exp) #calculating the cumulative sum  of variance explained \n",
    "print (\"Variance captured by each component is \\n\",var_exp)\n",
    "print(40 * '-')\n",
    "print (\"Cumulative variance captured as we travel each component \\n\",cum_var_exp)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "value=min(cum_var_exp, key=lambda x:abs(x-95)) # identifing the neareast value of 95 in the list \n",
    "\n",
    "n_components=list(cum_var_exp).index(value) # getting that nearest value of 95 and storing the index of that value in list\n",
    "#as no.of components\n",
    "\n",
    "\n",
    "from numpy import linalg as LA # importing the LA\n",
    "kernel=['linear','poly','rbf', 'sigmoid', 'cosine']  #giving the list of kernels in Kernelpca\n",
    "reconstruction_error=[] #empty list of reconstruction error\n",
    "for i in  kernel:\n",
    "    if kernel == ['poly','rbf', 'sigmoid']: #if these kernel's are comes then this gamma value has to choose \n",
    "        gamma=1 / (len(X.columns) * X.values.var())\n",
    "    else:\n",
    "        gamma=1/len(X.columns) #else the gamma values will be this \n",
    "    pca = KernelPCA(n_components=n_components,kernel=i,gamma=gamma,fit_inverse_transform=True)# deffing the kernelpca and \n",
    "    #the important parameters \n",
    "    pca2_results = pca.fit_transform(X_train) #fitting kernel pca \n",
    "    pca2_proj_back=pca.inverse_transform(pca2_results) #performing the inverse transform on the transformed fetaures with kernelpca\n",
    "    reconstruction_error.append(LA.norm((X_train, pca2_proj_back),None)) #calculating the difference between the orginal \n",
    "    #feature and inverse transformed features of kernel pca\n",
    "    \n",
    "print('reconstruction_error',reconstruction_error) #printing the reconstruction error\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=insurance.drop('charges',axis=1)\n",
    "y=insurance['charges']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='linear',gamma=1/len(X.columns)) # defing the kernelpca parameters\n",
    "# checking kernel parameter with linear and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score  #importing the classification report\n",
    "print('KernelPca Linear Model Results','\\n',r2_score(y_test, y_pred))#printing the classification report\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=insurance.drop('charges',axis=1)\n",
    "y=insurance['charges']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='poly',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with poly and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca poly Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=insurance.drop('charges',axis=1)\n",
    "y=insurance['charges']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='rbf',gamma=1 / (len(X.columns) * X.values.var())) # defing the kernelpca parameters\n",
    "# checking kernel parameter with rbf and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca rbf Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=insurance.drop('charges',axis=1)\n",
    "y=insurance['charges']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='sigmoid',gamma=1 / (len(X.columns) * X.values.var()))# defing the kernelpca parameters\n",
    "# checking kernel parameter with sigmoid and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca sigmoid Model Results','\\n',r2_score(y_test, y_pred))\n",
    "print('\\n')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "X=insurance.drop('charges',axis=1)\n",
    "y=insurance['charges']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #importing the Standardscaler\n",
    "sc = StandardScaler() # giving variable to Standardscaler \n",
    "X_train = sc.fit_transform(X) #fitting and transform standard scaler \n",
    "\n",
    "pca = KernelPCA(n_components=n_components,kernel='cosine',gamma=1 / (len(X.columns)))# defing the kernelpca parameters\n",
    "# checking kernel parameter with cosine and respective gamma value\n",
    "pca2_results = pca.fit_transform(X_train)\n",
    "\n",
    "columns=[] #empty list for columns from \n",
    "for i in range(n_components): #for loop n components\n",
    "    columns.append('KPCA'+str(i)) #appending the columns to the empty list\n",
    "df=pd.DataFrame(pca2_results,columns=columns) #creating the dataframe for kernelpca components\n",
    "df['target']=y #adding the target variable for the dataframe\n",
    "\n",
    "\n",
    "X=df.drop('target',axis=1) #defing the X features \n",
    "y=df['target'] # defining the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split #importing the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state =42) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  #importing the randomforest\n",
    "\n",
    "classifier= RandomForestRegressor() # giving the variable to randomforest\n",
    "classifier.fit(X_train, y_train) #fitting X and y \n",
    "y_pred= classifier.predict(X_test) #predicting the X test \n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('KernelPca cosine Model Results','\\n',r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d8329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
